# Cognitive-Behavioral-and-Social-Data

## Project Topic

This study evaluates the abstract reasoning capabilities of Claude 3, a recently developed large language
model, using the ConceptARC benchmark. Through systematic experimentation and enhanced prompting strategies,
we assess Claude 3â€™s performance across various analogical reasoning tasks within ConceptARC and compare it to
previous results for humans and GPT-4. Our findings reveal a mixed picture, with Claude 3 exhibiting proficiency in
certain conceptual areas while lagging behind in others. The study highlights the nuanced influence of prompting
techniques on model performance and contributes insights to the broader pursuit of artificial intelligence systems
capable of robust abstraction and reasoning. The discrepancies between Claude 3 and human-level performance
underscore the ongoing challenges in achieving human-parity in abstract reasoning within AI.

**Authors:** Davide Mazza, Thomas Rosso

**Email:** mazzadavide12@gmail.com

**Supervisor:** Giuseppe Sartori 
